{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eaf1e59",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron from Scratch\n",
    "This notebook extends our work by building a two-layer neural network using only NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c583a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051c1c1e",
   "metadata": {},
   "source": [
    "# Part 1: Revisiting Linear Regression with MSE Optimization\n",
    "Let's first revisit the MSE-optimized regression from Lab4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed71b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Load Boston housing dataset\n",
    "from sklearn.datasets import fetch_openml\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90237b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @TODO fetch boston dataset with correct parameters\n",
    "boston = fetch_openml(name=..., version=..., as_frame=...)\n",
    "df = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff27c631",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "# @TODO plot the data\n",
    "plt.scatter(..., ...)\n",
    "plt.xlabel('LSTAT (% lower status of the population)')\n",
    "plt.ylabel('MEDV (Median value of homes in $1000s)')\n",
    "plt.title('Boston Housing Dataset: LSTAT vs MEDV')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b7d468",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "# Define a neuron function for linear regression\n",
    "# @TODO implement the neuron function\n",
    "def neuron(x, w):\n",
    "    return ...  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9752947e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Assign initial weights\n",
    "# @TODO set weights\n",
    "w = np.array([...])\n",
    "print(f\"Initial weights: {w}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cf870b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "# Prepare data with bias term\n",
    "# @TODO create matrix X with feature and bias term\n",
    "X = np.vstack([..., ...]).T\n",
    "# @TODO get target values\n",
    "y = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadc24ce",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# MSE Optimization with Stochastic Gradient Descent\n",
    "current_weights = w.copy()\n",
    "learning_rate = 0.0001\n",
    "epochs = 100\n",
    "mse_history = []\n",
    "# Run stochastic gradient descent\n",
    "for epoch in range(epochs):\n",
    "    # Compute predictions and MSE for tracking\n",
    "    # @TODO compute predictions using your neuron function\n",
    "    y_pred = ...\n",
    "    # @TODO compute error\n",
    "    error = ...\n",
    "    # @TODO compute MSE\n",
    "    mse = ...\n",
    "    mse_history.append(mse)\n",
    "    \n",
    "    # Stochastic updates - process one point at a time\n",
    "    for j in range(len(df[\"LSTAT\"])):\n",
    "        # Get a single data point\n",
    "        # @TODO get single data point with bias\n",
    "        x_j = ...\n",
    "        y_j = ...\n",
    "        \n",
    "        # Compute prediction and gradient for this point\n",
    "        # @TODO compute prediction for this point\n",
    "        y_pred_j = ...\n",
    "        # @TODO compute gradient \n",
    "        gradient_j = ...\n",
    "        \n",
    "        # Update weights immediately\n",
    "        # @TODO update weights using gradient descent\n",
    "        current_weights = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ba55e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot MSE convergence\n",
    "plt.figure(figsize=(8, 5))\n",
    "# @TODO plot mse history\n",
    "plt.plot(..., ..., '...')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('MSE Convergence During Training')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e20e236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Final optimized model\n",
    "# @TODO compute final predictions\n",
    "y_pred_final = ...\n",
    "# @TODO compute final MSE\n",
    "final_mse = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef72eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(df['LSTAT'], df['MEDV'], alpha=0.7, label='Data')\n",
    "# @TODO plot the final prediction line\n",
    "plt.plot(..., ..., '...', label=f'Optimized Model (MSE: {final_mse:.2f})')\n",
    "plt.xlabel('LSTAT')\n",
    "plt.ylabel('MEDV')\n",
    "plt.title('Linear Regression with MSE Optimization')\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58875ae6",
   "metadata": {},
   "source": [
    "# Part 2: Building a Two-Layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c948cd8",
   "metadata": {},
   "source": [
    "Define a two-layer neural network\n",
    "First layer: 2 neurons\n",
    "Second layer: 2 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0523f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @TODO extract features and target\n",
    "X = data[['...']].values\n",
    "y = data[['...']].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50237f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "input_size = 1\n",
    "# @TODO set hidden size to 2 neurons as requested\n",
    "hidden_size = ...\n",
    "output_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff62aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @TODO initialize weights and biases for first layer\n",
    "theta1 = np.random.randn(..., ...) * ...\n",
    "bias1 = np.zeros((1, ...))\n",
    "# @TODO initialize weights and biases for second layer\n",
    "theta2 = np.random.randn(..., ...) * ...\n",
    "bias2 = np.zeros((1, ...))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eac74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "alpha = 0.0001  # Learning rate\n",
    "epochs = 10\n",
    "m = len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b76221",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0daaa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    for i in range(m):\n",
    "        # @TODO select random sample\n",
    "        rand_index = ...\n",
    "        # @TODO reshape sample for correct dimensions\n",
    "        x_i = ...\n",
    "        y_i = ...\n",
    "        \n",
    "        # Forward pass\n",
    "        # @TODO compute input to hidden layer\n",
    "        hidden_input = ...\n",
    "        # @TODO compute hidden layer output\n",
    "        hidden_output = ...  \n",
    "        # @TODO compute input to output layer\n",
    "        final_input = ...\n",
    "        # @TODO compute final output\n",
    "        final_output = ...\n",
    "        \n",
    "        # Compute error\n",
    "        # @TODO compute error\n",
    "        error = ...\n",
    "        \n",
    "        # Backpropagation\n",
    "        # @TODO compute gradient for output layer\n",
    "        d_final = ...\n",
    "        # @TODO compute gradient for theta2\n",
    "        d_theta2 = ...\n",
    "        # @TODO compute gradient for bias2\n",
    "        d_bias2 = ...\n",
    "        \n",
    "        # @TODO compute gradient for hidden layer\n",
    "        d_hidden = ...\n",
    "        # @TODO compute gradient for theta1\n",
    "        d_theta1 = ...\n",
    "        # @TODO compute gradient for bias1\n",
    "        d_bias1 = ...\n",
    "        \n",
    "        # Parameter updates\n",
    "        # @TODO update theta2\n",
    "        theta2 -= ...\n",
    "        # @TODO update bias2\n",
    "        bias2 -= ...\n",
    "        # @TODO update theta1\n",
    "        theta1 -= ...\n",
    "        # @TODO update bias1\n",
    "        bias1 -= ...\n",
    "\n",
    "    # @TODO compute MSE for this epoch\n",
    "    mse = ...\n",
    "    mse_history.append(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d5c5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot MSE convergence\n",
    "plt.figure(figsize=(8, 5))\n",
    "# @TODO plot MSE history\n",
    "plt.plot(..., ..., '...')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('MSE Convergence During Training')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa269adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "# @TODO compute hidden layer activations\n",
    "hidden_layer = ...\n",
    "# @TODO compute predictions\n",
    "predictions = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a903c62",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Plot results\n",
    "# @TODO plot data and predictions\n",
    "plt.scatter(..., ..., color='blue', label='Actual data')\n",
    "plt.scatter(..., ..., color='red', label='Neural Network Predictions')\n",
    "plt.xlabel('LSTAT (normalized)')\n",
    "plt.ylabel('MEDV (normalized)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f130d47",
   "metadata": {},
   "source": [
    "Now the same thing but using a more object-oriented approach\n",
    "Implementing a simple class-based neural network similar to PyTorch style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3447166",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize weights and biases\n",
    "        # @TODO initialize weights and biases with small random values\n",
    "        self.W1 = ...\n",
    "        self.b1 = ...\n",
    "        self.W2 = ...\n",
    "        self.b2 = ...\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward pass\n",
    "        # @TODO compute input to first layer\n",
    "        self.z1 = ...\n",
    "        # @TODO apply ReLU activation\n",
    "        self.a1 = ...\n",
    "        # @TODO compute output layer input\n",
    "        self.z2 = ...\n",
    "        return self.z2\n",
    "    \n",
    "    def backward(self, x, y, learning_rate):\n",
    "        # Backward pass\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Compute gradients\n",
    "        # @TODO compute error\n",
    "        error = ...\n",
    "        # @TODO compute gradient for W2\n",
    "        dW2 = ...\n",
    "        # @TODO compute gradient for b2\n",
    "        db2 = ...\n",
    "        \n",
    "        # @TODO compute gradient for hidden layer\n",
    "        d_hidden = ...\n",
    "        # @TODO apply ReLU derivative\n",
    "        d_hidden[...] = 0\n",
    "        \n",
    "        # @TODO compute gradient for W1\n",
    "        dW1 = ...\n",
    "        # @TODO compute gradient for b1\n",
    "        db1 = ...\n",
    "        \n",
    "        # Update parameters\n",
    "        # @TODO update W2 and b2\n",
    "        self.W2 -= ...\n",
    "        self.b2 -= ...\n",
    "        # @TODO update W1 and b1\n",
    "        self.W1 -= ...\n",
    "        self.b1 -= ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22e0a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Prepare normalized data for the neural network\n",
    "# @TODO extract features and reshape\n",
    "X_data = ...\n",
    "# @TODO extract target and reshape\n",
    "y_data = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ccb588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "# @TODO compute mean and std for normalization\n",
    "X_mean, X_std = ..., ...\n",
    "y_mean, y_std = ..., ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2173eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @TODO normalize data\n",
    "X_norm = ...\n",
    "y_norm = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5f1670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the model\n",
    "# @TODO create model with input_size=1, hidden_size=2, output_size=1\n",
    "model = TwoLayerNet(..., ..., ...)\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "batch_size = len(X_norm)  # Use all data at once (batch gradient descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec5862c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    # @TODO get model predictions\n",
    "    outputs = ...\n",
    "    \n",
    "    # Compute loss\n",
    "    # @TODO compute MSE loss\n",
    "    loss = ...\n",
    "    \n",
    "    # Backward pass and update\n",
    "    # @TODO perform backward pass and update weights\n",
    "    model.backward(..., ..., ...)\n",
    "    \n",
    "    # Print progress occasionally\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9458d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "# @TODO get model predictions\n",
    "predictions_oo = ...\n",
    "# @TODO denormalize predictions\n",
    "predictions_oo = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d4dbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(df['LSTAT'], df['MEDV'], alpha=0.7, label='Data')\n",
    "plt.plot(df['LSTAT'], y_pred_final, 'g-', label='Linear Model')\n",
    "# Sort for smooth line\n",
    "# @TODO sort indices for smooth plot\n",
    "sort_idx = ...\n",
    "# @TODO plot neural network predictions\n",
    "plt.plot(..., ..., '...', label='OO Neural Network')\n",
    "plt.xlabel('LSTAT')\n",
    "plt.ylabel('MEDV')\n",
    "plt.title('OO-Style Two-Layer Neural Network vs Linear Model')\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d098d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbde9017",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "# Now implementing the same network using PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c27b44",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define model architecture\n",
    "class TorchTwoLayerNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # @TODO initialize parent class\n",
    "        super(..., self).__init__()\n",
    "        # @TODO create first linear layer\n",
    "        self.layer1 = ...\n",
    "        # @TODO create ReLU activation\n",
    "        self.relu = ...\n",
    "        # @TODO create second linear layer\n",
    "        self.layer2 = ...\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        # @TODO apply first layer\n",
    "        x = ...\n",
    "        # @TODO apply ReLU activation\n",
    "        x = ...\n",
    "        # @TODO apply second layer\n",
    "        x = ...\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7175dad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "# @TODO convert X_norm to torch tensor\n",
    "X_tensor = ...\n",
    "# @TODO convert y_norm to torch tensor\n",
    "y_tensor = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60908d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model, loss function, and optimizer\n",
    "# @TODO create torch model with hidden_size=2\n",
    "torch_model = TorchTwoLayerNet(..., ..., ...)\n",
    "# @TODO create MSE loss criterion\n",
    "criterion = ...\n",
    "# @TODO create SGD optimizer\n",
    "optimizer = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00567664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(1000):\n",
    "    # Forward pass\n",
    "    # @TODO compute model outputs\n",
    "    outputs = ...\n",
    "    \n",
    "    # Compute loss\n",
    "    # @TODO compute loss between outputs and targets\n",
    "    loss = ...\n",
    "    \n",
    "    # Zero gradients, backward pass, and update\n",
    "    # @TODO clear existing gradients\n",
    "    ...\n",
    "    # @TODO compute gradients\n",
    "    ...\n",
    "    # @TODO update weights\n",
    "    ...\n",
    "    \n",
    "    # Print progress occasionally\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6fa1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "# @TODO set model to evaluation mode\n",
    "torch_model.eval()\n",
    "with torch.no_grad():\n",
    "    # @TODO get predictions from model\n",
    "    predictions_torch = ...\n",
    "    # @TODO denormalize predictions\n",
    "    predictions_torch = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee49eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all three models\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(df['LSTAT'], df['MEDV'], alpha=0.5, label='Data')\n",
    "plt.plot(df['LSTAT'], y_pred_final, 'g-', linewidth=2, label='Linear Model')\n",
    "# Sort for smooth lines\n",
    "sort_idx = np.argsort(df['LSTAT'].values)\n",
    "plt.plot(df['LSTAT'].values[sort_idx], predictions_oo[sort_idx], 'r--', linewidth=2, label='NumPy NN')\n",
    "# @TODO plot torch model predictions\n",
    "plt.plot(..., ..., '...', linewidth=2, label='PyTorch NN')\n",
    "plt.xlabel('LSTAT')\n",
    "plt.ylabel('MEDV')\n",
    "plt.title('Comparing Different Neural Network Implementations')\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
